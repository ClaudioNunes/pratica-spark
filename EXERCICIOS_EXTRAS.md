# Exerc√≠cios Extras - PySpark Lab

Exerc√≠cios adicionais para aprofundar seus conhecimentos em PySpark e processamento de Big Data.

---

## Exerc√≠cio Extra 1: An√°lise de Cohort

**N√≠vel:** Intermedi√°rio  
**Tempo estimado:** 45 minutos

### Objetivo
Implementar uma an√°lise de cohort para identificar padr√µes de reten√ß√£o de clientes ao longo do tempo.

### Descri√ß√£o
Uma an√°lise de cohort agrupa clientes pela data da primeira compra e acompanha seu comportamento ao longo dos meses subsequentes.

### Tarefas

1. Identifique a primeira compra de cada cliente
2. Agrupe clientes por m√™s da primeira compra (cohort)
3. Calcule quantos clientes de cada cohort fizeram compras nos meses seguintes
4. Calcule a taxa de reten√ß√£o mensal

### Dicas

```python
from pyspark.sql.functions import min, max, months_between, floor

# Primeira compra
first_purchase = df.groupBy("customer_id").agg(
    min("date").alias("first_purchase_date")
)

# Join com dados originais
df_with_cohort = df.join(first_purchase, "customer_id")

# Calcule meses desde primeira compra
df_with_cohort = df_with_cohort.withColumn(
    "months_since_first",
    floor(months_between(col("date"), col("first_purchase_date")))
)
```

### Resultado Esperado
Tabela mostrando reten√ß√£o por cohort:

```
Cohort     | Month 0 | Month 1 | Month 2 | Month 3 |
2024-01    |   100%  |   45%   |   32%   |   28%   |
2024-02    |   100%  |   52%   |   38%   |    -    |
```

---

## Exerc√≠cio Extra 2: Detec√ß√£o de Anomalias

**N√≠vel:** Avan√ßado  
**Tempo estimado:** 60 minutos

### Objetivo
Implementar um sistema simples de detec√ß√£o de anomalias em vendas usando estat√≠sticas descritivas.

### Descri√ß√£o
Identifique transa√ß√µes an√¥malas (outliers) baseadas em:
- Valor da transa√ß√£o muito alto ou baixo
- Quantidade incomum de produtos
- Padr√µes incomuns por regi√£o

### Tarefas

1. Calcule m√©dia e desvio padr√£o da receita
2. Identifique transa√ß√µes fora de 3 desvios padr√£o
3. Analise padr√µes an√¥malos por categoria
4. Gere relat√≥rio de alertas

### Dicas

```python
from pyspark.sql.functions import mean, stddev

# Estat√≠sticas
stats = df.select(
    mean("revenue").alias("avg_revenue"),
    stddev("revenue").alias("std_revenue")
).first()

# Identifica outliers
outliers = df.filter(
    (col("revenue") > stats.avg_revenue + 3 * stats.std_revenue) |
    (col("revenue") < stats.avg_revenue - 3 * stats.std_revenue)
)
```

### Resultado Esperado
- Lista de transa√ß√µes an√¥malas
- Estat√≠sticas de anomalias por categoria
- Visualiza√ß√£o de distribui√ß√£o

---

## Exerc√≠cio Extra 3: An√°lise de RFM

**N√≠vel:** Intermedi√°rio  
**Tempo estimado:** 45 minutos

### Objetivo
Implementar an√°lise RFM (Recency, Frequency, Monetary) para segmenta√ß√£o de clientes.

### Descri√ß√£o
RFM √© uma t√©cnica de marketing que segmenta clientes baseado em:
- **Recency**: Qu√£o recentemente o cliente comprou
- **Frequency**: Com que frequ√™ncia o cliente compra
- **Monetary**: Quanto o cliente gastou

### Tarefas

1. Calcule R, F, M para cada cliente
2. Crie scores de 1-5 para cada m√©trica
3. Combine scores para criar segmentos
4. Gere insights sobre cada segmento

### Dicas

```python
from pyspark.sql.functions import datediff, current_date, ntile
from pyspark.sql.window import Window

# Recency (dias desde √∫ltima compra)
rfm = df.groupBy("customer_id").agg(
    datediff(current_date(), max("date")).alias("recency"),
    count("*").alias("frequency"),
    sum("revenue").alias("monetary")
)

# Cria scores usando ntile (quintis)
window = Window.orderBy("recency")
rfm = rfm.withColumn("R_score", ntile(5).over(window))
```

### Resultado Esperado
Segmentos como:
- **Champions** (R=5, F=5, M=5): Melhores clientes
- **At Risk** (R=1, F=5, M=5): Clientes valiosos que n√£o compram h√° tempo
- **Lost** (R=1, F=1, M=1): Clientes perdidos

---

## Exerc√≠cio Extra 4: Market Basket Analysis

**N√≠vel:** Avan√ßado  
**Tempo estimado:** 90 minutos

### Objetivo
Implementar an√°lise de cesta de compras para identificar produtos frequentemente comprados juntos.

### Descri√ß√£o
An√°lise de associa√ß√£o entre produtos usando m√©tricas:
- **Support**: Frequ√™ncia da combina√ß√£o
- **Confidence**: P(B|A) - probabilidade de comprar B dado que comprou A
- **Lift**: Quanto a compra de A aumenta a probabilidade de comprar B

### Tarefas

1. Agrupe produtos por transa√ß√£o (mesmo cliente, mesma data)
2. Encontre pares de produtos frequentemente comprados juntos
3. Calcule support, confidence e lift
4. Identifique as associa√ß√µes mais fortes

### Dicas

```python
# Agrupa por transa√ß√£o
baskets = df.groupBy("customer_id", "date").agg(
    collect_list("product_id").alias("products")
)

# Self-join para encontrar pares
# Pode usar MLlib FPGrowth para an√°lise mais sofisticada
from pyspark.ml.fpm import FPGrowth

fp = FPGrowth(itemsCol="products", minSupport=0.05, minConfidence=0.3)
model = fp.fit(baskets)

# Frequent itemsets
model.freqItemsets.show()

# Association rules
model.associationRules.show()
```

### Resultado Esperado
```
Antecedent    | Consequent   | Confidence | Lift  |
[Notebook]    | [Mouse]      |    0.75    |  2.3  |
[Mouse]       | [Mouse Pad]  |    0.60    |  1.8  |
```

---

## Exerc√≠cio Extra 5: Time Series Analysis

**N√≠vel:** Avan√ßado  
**Tempo estimado:** 60 minutos

### Objetivo
An√°lise de s√©ries temporais com tend√™ncias e sazonalidade.

### Tarefas

1. Calcule vendas di√°rias agregadas
2. Calcule m√©dia m√≥vel (7 dias, 30 dias)
3. Identifique tend√™ncias de crescimento
4. Detecte sazonalidade (dia da semana)

### Dicas

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import avg, lag, lead

# Vendas di√°rias
daily_sales = df.groupBy("date").agg(
    sum("revenue").alias("daily_revenue")
).orderBy("date")

# M√©dia m√≥vel 7 dias
window_7d = Window.orderBy("date").rowsBetween(-6, 0)
daily_sales = daily_sales.withColumn(
    "ma_7d",
    avg("daily_revenue").over(window_7d)
)

# Crescimento dia a dia
window_lag = Window.orderBy("date")
daily_sales = daily_sales.withColumn(
    "prev_day",
    lag("daily_revenue").over(window_lag)
).withColumn(
    "growth_rate",
    (col("daily_revenue") - col("prev_day")) / col("prev_day") * 100
)
```

---

## Exerc√≠cio Extra 6: An√°lise Geoespacial

**N√≠vel:** Intermedi√°rio  
**Tempo estimado:** 40 minutos

### Objetivo
An√°lise avan√ßada por regi√£o com m√©tricas comparativas.

### Tarefas

1. Calcule market share de cada regi√£o
2. Compare performance entre regi√µes
3. Identifique regi√µes com maior potencial de crescimento
4. Analise mix de produtos por regi√£o

### Dicas

```python
# Market share
total_revenue = df.select(sum("revenue")).first()[0]

region_analysis = df.groupBy("region").agg(
    sum("revenue").alias("region_revenue"),
    count("*").alias("num_transactions")
).withColumn(
    "market_share",
    (col("region_revenue") / total_revenue) * 100
)

# Performance vs m√©dia
avg_revenue = df.select(avg("revenue")).first()[0]

region_analysis = region_analysis.withColumn(
    "performance_index",
    col("region_revenue") / col("num_transactions") / avg_revenue
)
```

---

## Exerc√≠cio Extra 7: Customer Lifetime Value (CLV)

**N√≠vel:** Avan√ßado  
**Tempo estimado:** 75 minutos

### Objetivo
Calcular o Customer Lifetime Value de cada cliente.

### Descri√ß√£o
CLV = (Valor m√©dio de compra) √ó (Frequ√™ncia de compra) √ó (Tempo como cliente)

### Tarefas

1. Calcule m√©tricas por cliente:
   - Valor m√©dio de compra
   - Frequ√™ncia de compra (compras/m√™s)
   - Tempo como cliente (dias)
2. Estime CLV para cada cliente
3. Segmente clientes por CLV
4. Identifique caracter√≠sticas de alto CLV

### Dicas

```python
from pyspark.sql.functions import datediff

customer_metrics = df.groupBy("customer_id").agg(
    avg("revenue").alias("avg_purchase_value"),
    count("*").alias("num_purchases"),
    datediff(max("date"), min("date")).alias("customer_lifetime_days")
)

# CLV simplificado
customer_metrics = customer_metrics.withColumn(
    "purchase_frequency",
    col("num_purchases") / (col("customer_lifetime_days") / 30)
).withColumn(
    "clv",
    col("avg_purchase_value") * col("purchase_frequency") * 
    (col("customer_lifetime_days") / 365)
)
```

---

## Exerc√≠cio Extra 8: Otimiza√ß√£o de Performance

**N√≠vel:** Avan√ßado  
**Tempo estimado:** 60 minutos

### Objetivo
Otimizar queries Spark para melhor performance.

### Tarefas

1. Execute a an√°lise de vendas com diferentes configura√ß√µes
2. Compare tempos de execu√ß√£o
3. Analise planos de execu√ß√£o (explain)
4. Aplique otimiza√ß√µes

### Configura√ß√µes para testar:

```python
# Teste 1: Padr√£o
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# Teste 2: Otimizado
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "8") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# Teste 3: Com cache
df.cache()
df.count()  # Materializa cache

# Teste 4: Com reparticionamento
df_repartitioned = df.repartition(8, "category")
```

### M√©tricas para coletar:
- Tempo de execu√ß√£o total
- Tempo por stage
- Shuffle read/write
- Mem√≥ria utilizada

---

## Exerc√≠cio Extra 9: Join Optimization

**N√≠vel:** Avan√ßado  
**Tempo estimado:** 45 minutos

### Objetivo
Entender e otimizar joins no Spark.

### Tarefas

1. Crie um dataset de produtos (se ainda n√£o existir)
2. Fa√ßa join entre vendas e produtos
3. Compare diferentes tipos de join:
   - Broadcast join (pequeno + grande)
   - Sort-merge join (grande + grande)
4. Analise planos de execu√ß√£o

### C√≥digo:

```python
# Dataset de produtos (pequeno)
products_df = spark.read.csv("data/products.csv", header=True)

# Join regular
result = df.join(products_df, "product_id")

# Broadcast join (for√ßa)
from pyspark.sql.functions import broadcast
result_broadcast = df.join(broadcast(products_df), "product_id")

# Compare planos
result.explain()
result_broadcast.explain()
```

---

## Exerc√≠cio Extra 10: Data Quality Assessment

**N√≠vel:** Intermedi√°rio  
**Tempo estimado:** 45 minutos

### Objetivo
Implementar checks de qualidade de dados.

### Tarefas

1. Verificar valores nulos/missing
2. Identificar duplicatas
3. Validar ranges de valores
4. Verificar consist√™ncia de dados
5. Gerar relat√≥rio de qualidade

### C√≥digo:

```python
# Valores nulos por coluna
df.select([
    count(when(col(c).isNull(), c)).alias(c)
    for c in df.columns
]).show()

# Duplicatas
duplicates = df.groupBy("transaction_id").count().filter(col("count") > 1)

# Valida√ß√µes de neg√≥cio
quality_checks = df.agg(
    count(when(col("price") < 0, True)).alias("negative_prices"),
    count(when(col("quantity") < 1, True)).alias("invalid_quantity"),
    count(when(col("date") > current_date(), True)).alias("future_dates")
)

quality_checks.show()
```

---

## Dicas Gerais para Todos os Exerc√≠cios

### Boas Pr√°ticas:

1. **Sempre use `.explain()`** para entender o plano de execu√ß√£o
2. **Cache DataFrames** que ser√£o reutilizados
3. **Particione adequadamente** para joins e agrega√ß√µes
4. **Use fun√ß√µes built-in** ao inv√©s de UDFs quando poss√≠vel
5. **Monitore mem√≥ria** e ajuste configura√ß√µes conforme necess√°rio

### Debugging:

```python
# Conta linhas em cada stage
df.cache()
print(f"Total records: {df.count()}")

# Mostra plano f√≠sico
df.explain(mode="formatted")

# Verifica parti√ß√µes
print(f"Number of partitions: {df.rdd.getNumPartitions()}")

# Mostra schema
df.printSchema()
```

### Performance Tips:

1. Use `filter()` antes de `groupBy()`
2. Use `select()` para escolher apenas colunas necess√°rias
3. Evite `collect()` em datasets grandes
4. Use `limit()` durante desenvolvimento/testes
5. Configure `spark.sql.shuffle.partitions` adequadamente

---

## Recursos Adicionais

### Datasets P√∫blicos para Pr√°tica:

1. **Kaggle**: Diversos datasets de vendas e e-commerce
2. **UCI Machine Learning Repository**: Datasets educacionais
3. **AWS Open Data**: Datasets p√∫blicos na nuvem
4. **Google Public Datasets**: BigQuery public datasets

### Ferramentas de Visualiza√ß√£o:

- **Matplotlib/Seaborn**: Gr√°ficos em Python
- **Plotly**: Visualiza√ß√µes interativas
- **Tableau/Power BI**: BI profissional
- **Spark UI**: Monitoramento de jobs (localhost:4040)

---

## Crit√©rios de Avalia√ß√£o

Para cada exerc√≠cio, considere:

| Crit√©rio | Peso | Descri√ß√£o |
|----------|------|-----------|
| **Corre√ß√£o** | 40% | C√≥digo funciona e produz resultado correto |
| **Performance** | 20% | Uso eficiente de recursos Spark |
| **C√≥digo Limpo** | 20% | Legibilidade e organiza√ß√£o |
| **Documenta√ß√£o** | 10% | Coment√°rios e explica√ß√µes |
| **Insights** | 10% | Qualidade das conclus√µes |

---

## Entrega dos Exerc√≠cios

1. Crie uma branch para seus exerc√≠cios: `git checkout -b exercicios-extras`
2. Implemente cada exerc√≠cio em arquivo separado: `exercicio_01.py`, etc.
3. Documente resultados em `RESULTADOS_EXERCICIOS.md`
4. Commit e push: `git push origin exercicios-extras`
5. (Opcional) Crie Pull Request para revis√£o

---

**Boa sorte com os exerc√≠cios!** üöÄ

Se tiver d√∫vidas, consulte a documenta√ß√£o oficial do PySpark ou entre em contato com o instrutor.
