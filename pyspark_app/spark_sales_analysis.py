#!/usr/bin/env python3
"""
AnÃ¡lise de Vendas de E-commerce com PySpark
Demonstra anÃ¡lises complexas usando DataFrames e SQL
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, sum, count, avg, max, min, round, desc, asc,
    year, month, dayofweek, date_format, to_date, expr
)
from pyspark.sql.window import Window
import time

def create_spark_session():
    """Cria e configura SparkSession com configuraÃ§Ãµes otimizadas"""
    return SparkSession.builder \
        .appName("SalesAnalysis-PySpark") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.sql.shuffle.partitions", "8") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()

def load_data(spark, file_path):
    """
    Carrega dados de vendas do CSV
    Infere schema automaticamente e valida dados
    """
    print("\nğŸ“‚ Carregando dados...")
    
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(file_path)
    
    # Converte coluna date para tipo date
    df = df.withColumn("date", to_date(col("date")))
    
    # Adiciona coluna de receita
    df = df.withColumn("revenue", col("quantity") * col("price"))
    
    print(f"âœ… {df.count()} transaÃ§Ãµes carregadas com sucesso!")
    
    return df

def explore_data(df):
    """ExploraÃ§Ã£o inicial dos dados"""
    print("\n" + "="*60)
    print("ğŸ“Š EXPLORAÃ‡ÃƒO INICIAL DOS DADOS")
    print("="*60)
    
    # Schema
    print("\nğŸ“‹ Schema do DataFrame:")
    df.printSchema()
    
    # Primeiras linhas
    print("\nğŸ“„ Primeiras 10 linhas:")
    df.show(10, truncate=False)
    
    # EstatÃ­sticas descritivas
    print("\nğŸ“ˆ EstatÃ­sticas Descritivas:")
    df.select("quantity", "price", "revenue").describe().show()
    
    # Contagem de valores Ãºnicos
    print("\nğŸ”¢ Valores Ãšnicos:")
    print(f"   â€¢ Clientes: {df.select('customer_id').distinct().count()}")
    print(f"   â€¢ Produtos: {df.select('product_id').distinct().count()}")
    print(f"   â€¢ Categorias: {df.select('category').distinct().count()}")
    print(f"   â€¢ RegiÃµes: {df.select('region').distinct().count()}")

def analysis_revenue_by_category(df):
    """
    AnÃ¡lise 1: Receita Total por Categoria
    Identifica as categorias mais lucrativas
    """
    print("\n" + "="*60)
    print("ğŸ’° ANÃLISE 1: RECEITA POR CATEGORIA")
    print("="*60)
    
    result = df.groupBy("category") \
        .agg(
            sum("revenue").alias("total_revenue"),
            count("*").alias("num_transactions"),
            avg("revenue").alias("avg_revenue"),
            sum("quantity").alias("total_quantity")
        ) \
        .withColumn("total_revenue", round(col("total_revenue"), 2)) \
        .withColumn("avg_revenue", round(col("avg_revenue"), 2)) \
        .orderBy(desc("total_revenue"))
    
    print("\nğŸ“Š Receita por Categoria:")
    result.show(truncate=False)
    
    # Salva resultado
    result.write.mode("overwrite").csv("data/output/revenue_by_category", header=True)
    print("ğŸ’¾ Resultado salvo em: data/output/revenue_by_category")
    
    return result

def analysis_top_products(df):
    """
    AnÃ¡lise 2: Top 10 Produtos Mais Vendidos
    Identifica os produtos mais populares
    """
    print("\n" + "="*60)
    print("ğŸ† ANÃLISE 2: TOP 10 PRODUTOS MAIS VENDIDOS")
    print("="*60)
    
    result = df.groupBy("product_id", "product_name", "category") \
        .agg(
            sum("quantity").alias("total_sold"),
            sum("revenue").alias("total_revenue"),
            count("*").alias("num_transactions")
        ) \
        .withColumn("total_revenue", round(col("total_revenue"), 2)) \
        .orderBy(desc("total_sold")) \
        .limit(10)
    
    print("\nğŸ¥‡ Top 10 Produtos:")
    result.show(truncate=False)
    
    result.write.mode("overwrite").csv("data/output/top_products", header=True)
    print("ğŸ’¾ Resultado salvo em: data/output/top_products")
    
    return result

def analysis_sales_by_region(df):
    """
    AnÃ¡lise 3: Vendas por RegiÃ£o
    Analisa a distribuiÃ§Ã£o geogrÃ¡fica das vendas
    """
    print("\n" + "="*60)
    print("ğŸ—ºï¸  ANÃLISE 3: VENDAS POR REGIÃƒO")
    print("="*60)
    
    result = df.groupBy("region") \
        .agg(
            count("*").alias("num_transactions"),
            sum("revenue").alias("total_revenue"),
            avg("revenue").alias("avg_transaction_value"),
            sum("quantity").alias("total_items_sold")
        ) \
        .withColumn("total_revenue", round(col("total_revenue"), 2)) \
        .withColumn("avg_transaction_value", round(col("avg_transaction_value"), 2)) \
        .orderBy(desc("total_revenue"))
    
    print("\nğŸ“ Vendas por RegiÃ£o:")
    result.show(truncate=False)
    
    # Calcula participaÃ§Ã£o percentual
    total_revenue = df.select(sum("revenue")).first()[0]
    
    result_with_percentage = result.withColumn(
        "percentage",
        round((col("total_revenue") / total_revenue) * 100, 2)
    )
    
    print("\nğŸ“Š ParticipaÃ§Ã£o Percentual:")
    result_with_percentage.select("region", "total_revenue", "percentage").show()
    
    result.write.mode("overwrite").csv("data/output/sales_by_region", header=True)
    print("ğŸ’¾ Resultado salvo em: data/output/sales_by_region")
    
    return result

def analysis_customer_metrics(df):
    """
    AnÃ¡lise 4: MÃ©tricas por Cliente
    Calcula ticket mÃ©dio e identifica clientes VIP
    """
    print("\n" + "="*60)
    print("ğŸ‘¥ ANÃLISE 4: MÃ‰TRICAS POR CLIENTE")
    print("="*60)
    
    # AgregaÃ§Ã£o por cliente
    customer_metrics = df.groupBy("customer_id") \
        .agg(
            count("*").alias("num_purchases"),
            sum("revenue").alias("total_spent"),
            avg("revenue").alias("avg_ticket"),
            max("date").alias("last_purchase")
        ) \
        .withColumn("total_spent", round(col("total_spent"), 2)) \
        .withColumn("avg_ticket", round(col("avg_ticket"), 2))
    
    # Top 20 clientes
    top_customers = customer_metrics \
        .orderBy(desc("total_spent")) \
        .limit(20)
    
    print("\nğŸ’ Top 20 Clientes (por valor total):")
    top_customers.show(truncate=False)
    
    # EstatÃ­sticas gerais
    print("\nğŸ“Š EstatÃ­sticas Gerais de Clientes:")
    customer_metrics.select("num_purchases", "total_spent", "avg_ticket") \
        .describe().show()
    
    # SegmentaÃ§Ã£o de clientes
    print("\nğŸ¯ SegmentaÃ§Ã£o de Clientes:")
    segments = customer_metrics.groupBy(
        expr("CASE WHEN total_spent >= 5000 THEN 'VIP' " +
             "WHEN total_spent >= 2000 THEN 'Premium' " +
             "WHEN total_spent >= 500 THEN 'Regular' " +
             "ELSE 'Basic' END").alias("segment")
    ).agg(
        count("*").alias("num_customers"),
        round(avg("total_spent"), 2).alias("avg_spent")
    ).orderBy(desc("avg_spent"))
    
    segments.show()
    
    customer_metrics.write.mode("overwrite").csv("data/output/customer_metrics", header=True)
    print("ğŸ’¾ Resultado salvo em: data/output/customer_metrics")
    
    return customer_metrics

def analysis_temporal_trends(df):
    """
    AnÃ¡lise 5: TendÃªncias Temporais
    Analisa padrÃµes de vendas ao longo do tempo
    """
    print("\n" + "="*60)
    print("ğŸ“… ANÃLISE 5: TENDÃŠNCIAS TEMPORAIS")
    print("="*60)
    
    # Adiciona colunas temporais
    df_with_time = df.withColumn("year", year("date")) \
        .withColumn("month", month("date")) \
        .withColumn("day_of_week", dayofweek("date"))
    
    # Vendas por mÃªs
    print("\nğŸ“† Vendas por MÃªs:")
    monthly_sales = df_with_time.groupBy("year", "month") \
        .agg(
            sum("revenue").alias("total_revenue"),
            count("*").alias("num_transactions")
        ) \
        .withColumn("total_revenue", round(col("total_revenue"), 2)) \
        .orderBy("year", "month")
    
    monthly_sales.show()
    
    # Vendas por dia da semana
    print("\nğŸ“Š Vendas por Dia da Semana:")
    daily_pattern = df_with_time.groupBy("day_of_week") \
        .agg(
            sum("revenue").alias("total_revenue"),
            count("*").alias("num_transactions"),
            avg("revenue").alias("avg_transaction")
        ) \
        .withColumn("total_revenue", round(col("total_revenue"), 2)) \
        .withColumn("avg_transaction", round(col("avg_transaction"), 2)) \
        .orderBy("day_of_week")
    
    # Mapeia nÃºmeros para nomes dos dias
    daily_pattern.withColumn(
        "day_name",
        expr("CASE day_of_week " +
             "WHEN 1 THEN 'Domingo' " +
             "WHEN 2 THEN 'Segunda' " +
             "WHEN 3 THEN 'TerÃ§a' " +
             "WHEN 4 THEN 'Quarta' " +
             "WHEN 5 THEN 'Quinta' " +
             "WHEN 6 THEN 'Sexta' " +
             "WHEN 7 THEN 'SÃ¡bado' END")
    ).select("day_name", "num_transactions", "total_revenue", "avg_transaction").show()
    
    monthly_sales.write.mode("overwrite").csv("data/output/monthly_trends", header=True)
    print("ğŸ’¾ Resultado salvo em: data/output/monthly_trends")
    
    return monthly_sales

def analysis_product_performance(df):
    """
    AnÃ¡lise 6: Performance de Produtos
    AnÃ¡lise detalhada por produto e categoria
    """
    print("\n" + "="*60)
    print("ğŸ“¦ ANÃLISE 6: PERFORMANCE DE PRODUTOS")
    print("="*60)
    
    # Cria view temporÃ¡ria para SQL
    df.createOrReplaceTempView("sales")
    
    # Query SQL complexa
    query = """
        SELECT 
            category,
            product_name,
            COUNT(*) as num_sales,
            SUM(quantity) as total_quantity,
            ROUND(SUM(revenue), 2) as total_revenue,
            ROUND(AVG(price), 2) as avg_price,
            ROUND(AVG(quantity), 2) as avg_quantity_per_sale
        FROM sales
        GROUP BY category, product_name
        ORDER BY category, total_revenue DESC
    """
    
    result = df.sparkSession.sql(query)
    
    print("\nğŸ“Š Performance Detalhada por Produto:")
    result.show(50, truncate=False)
    
    result.write.mode("overwrite").csv("data/output/product_performance", header=True)
    print("ğŸ’¾ Resultado salvo em: data/output/product_performance")
    
    return result

def generate_executive_summary(df):
    """
    Gera resumo executivo com KPIs principais
    """
    print("\n" + "="*60)
    print("ğŸ“Š RESUMO EXECUTIVO - KPIS PRINCIPAIS")
    print("="*60)
    
    # Calcula KPIs
    total_revenue = df.select(sum("revenue")).first()[0]
    total_transactions = df.count()
    avg_ticket = df.select(avg("revenue")).first()[0]
    num_customers = df.select("customer_id").distinct().count()
    num_products = df.select("product_id").distinct().count()
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    KPIS PRINCIPAIS                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ’° Receita Total:          R$ {total_revenue:>15,.2f}   â•‘
â•‘  ğŸ›’ Total de TransaÃ§Ãµes:       {total_transactions:>15,}   â•‘
â•‘  ğŸ« Ticket MÃ©dio:           R$ {avg_ticket:>15,.2f}   â•‘
â•‘  ğŸ‘¥ Clientes Ãšnicos:           {num_customers:>15,}   â•‘
â•‘  ğŸ“¦ Produtos Ãšnicos:           {num_products:>15,}   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

def main():
    """FunÃ§Ã£o principal - orquestra todas as anÃ¡lises"""
    print("=" * 60)
    print("  ANÃLISE DE VENDAS COM PYSPARK")
    print("  Sistema de Business Intelligence para E-commerce")
    print("=" * 60)
    
    start_time = time.time()
    
    # Cria SparkSession
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"\nâš™ï¸  ConfiguraÃ§Ã£o do Spark:")
    print(f"   â€¢ VersÃ£o: {spark.version}")
    print(f"   â€¢ Cores: {spark.sparkContext.defaultParallelism}")
    print(f"   â€¢ MemÃ³ria Driver: 2GB")
    print(f"   â€¢ MemÃ³ria Executor: 2GB")
    
    try:
        # Arquivo de entrada
        input_file = "data/sales_data.csv"
        
        # Carrega dados
        df = load_data(spark, input_file)
        
        # Cache do DataFrame para melhor performance
        df.cache()
        
        # ExploraÃ§Ã£o inicial
        explore_data(df)
        
        # Executa anÃ¡lises
        analysis_revenue_by_category(df)
        analysis_top_products(df)
        analysis_sales_by_region(df)
        analysis_customer_metrics(df)
        analysis_temporal_trends(df)
        analysis_product_performance(df)
        
        # Resumo executivo
        generate_executive_summary(df)
        
        # Tempo total
        elapsed_time = time.time() - start_time
        
        print("\n" + "="*60)
        print("âœ… ANÃLISE CONCLUÃDA COM SUCESSO!")
        print("="*60)
        print(f"\nâ±ï¸  Tempo total de execuÃ§Ã£o: {elapsed_time:.2f} segundos")
        print(f"ğŸ“ Resultados salvos em: data/output/")
        
        print("\nğŸ“š Conceitos PySpark demonstrados:")
        print("   â€¢ DataFrames e Schema Inference")
        print("   â€¢ TransformaÃ§Ãµes (groupBy, agg, join)")
        print("   â€¢ FunÃ§Ãµes de agregaÃ§Ã£o (sum, avg, count)")
        print("   â€¢ FunÃ§Ãµes de janela (Window)")
        print("   â€¢ Spark SQL e views temporÃ¡rias")
        print("   â€¢ Lazy Evaluation e Cache")
        print("   â€¢ GravaÃ§Ã£o de resultados em CSV")
        
    except FileNotFoundError:
        print(f"\nâŒ Erro: Arquivo nÃ£o encontrado!")
        print("ğŸ’¡ Execute primeiro: python3 data_generator.py")
    except Exception as e:
        print(f"\nâŒ Erro durante anÃ¡lise: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Encerra SparkSession
        spark.stop()
        print("\nğŸ”š SparkSession encerrada.")

if __name__ == "__main__":
    main()
