#!/usr/bin/env python3
"""
Gerador de Dados de Vendas para An√°lise com PySpark
Gera um dataset sint√©tico de transa√ß√µes de e-commerce
"""

import csv
import random
from datetime import datetime, timedelta
from pathlib import Path

# Configura√ß√µes
NUM_TRANSACTIONS = 1000
OUTPUT_FILE = "data/sales_data.csv"

# Dados fict√≠cios em Portugu√™s Brasileiro
PRODUCTS = [
    ("P001", "Notebook Dell", "Eletr√¥nicos", 2800.00),
    ("P002", "Mouse Logitech", "Eletr√¥nicos", 45.00),
    ("P003", "Teclado Mec√¢nico", "Eletr√¥nicos", 250.00),
    ("P004", "Monitor LG 24\"", "Eletr√¥nicos", 850.00),
    ("P005", "Webcam HD", "Eletr√¥nicos", 180.00),
    ("P006", "Fone Bluetooth", "Eletr√¥nicos", 120.00),
    ("P007", "SSD 500GB", "Eletr√¥nicos", 350.00),
    ("P008", "Livro Python", "Livros", 45.00),
    ("P009", "Livro Ci√™ncia de Dados", "Livros", 65.00),
    ("P010", "Livro Machine Learning", "Livros", 80.00),
    ("P011", "Caderno", "Papelaria", 15.00),
    ("P012", "Caneta Pack 10", "Papelaria", 12.00),
    ("P013", "Mochila Executiva", "Acess√≥rios", 180.00),
    ("P014", "Garrafa T√©rmica", "Acess√≥rios", 45.00),
    ("P015", "Tablet Samsung", "Eletr√¥nicos", 1200.00),
    ("P016", "SmartWatch", "Eletr√¥nicos", 800.00),
    ("P017", "Carregador USB-C", "Eletr√¥nicos", 35.00),
    ("P018", "Hub USB", "Eletr√¥nicos", 60.00),
    ("P019", "Cabo HDMI 2m", "Eletr√¥nicos", 25.00),
    ("P020", "Mouse Pad", "Acess√≥rios", 20.00),
]

REGIONS = ["Sudeste", "Sul", "Nordeste", "Norte", "Centro-Oeste"]

# Probabilidades para tornar dados mais realistas
CATEGORY_WEIGHTS = {
    "Eletr√¥nicos": 0.5,
    "Livros": 0.2,
    "Papelaria": 0.15,
    "Acess√≥rios": 0.15,
}

def generate_transaction_id(index):
    """Gera ID √∫nico para transa√ß√£o"""
    return f"TX{index:06d}"

def generate_customer_id():
    """Gera ID de cliente"""
    return f"C{random.randint(100, 999)}"

def generate_date(start_date, end_date):
    """Gera data aleat√≥ria no intervalo"""
    delta = end_date - start_date
    random_days = random.randint(0, delta.days)
    return (start_date + timedelta(days=random_days)).strftime("%Y-%m-%d")

def select_product():
    """Seleciona produto com distribui√ß√£o ponderada"""
    # Filtra produtos por categoria e aplica pesos
    category = random.choices(
        list(CATEGORY_WEIGHTS.keys()),
        weights=list(CATEGORY_WEIGHTS.values())
    )[0]
    
    category_products = [p for p in PRODUCTS if p[2] == category]
    return random.choice(category_products)

def generate_quantity(product_price):
    """Gera quantidade baseada no pre√ßo (produtos mais caros: menor quantidade)"""
    if product_price > 1000:
        return random.randint(1, 2)
    elif product_price > 200:
        return random.randint(1, 3)
    else:
        return random.randint(1, 10)

def generate_sales_data():
    """Gera dataset de vendas"""
    print("üîÑ Gerando dados de vendas...")
    
    # Cria diret√≥rio se n√£o existir
    Path("data").mkdir(exist_ok=True)
    
    # Define intervalo de datas (√∫ltimos 6 meses)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=180)
    
    # Gera transa√ß√µes
    transactions = []
    
    for i in range(1, NUM_TRANSACTIONS + 1):
        transaction_id = generate_transaction_id(i)
        date = generate_date(start_date, end_date)
        customer_id = generate_customer_id()
        
        # Seleciona produto
        product_id, product_name, category, price = select_product()
        
        # Gera quantidade
        quantity = generate_quantity(price)
        
        # Seleciona regi√£o
        region = random.choice(REGIONS)
        
        transactions.append([
            transaction_id,
            date,
            customer_id,
            product_id,
            product_name,
            category,
            quantity,
            f"{price:.2f}",
            region
        ])
    
    # Salva em CSV
    with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # Header
        writer.writerow([
            "transaction_id",
            "date",
            "customer_id",
            "product_id",
            "product_name",
            "category",
            "quantity",
            "price",
            "region"
        ])
        
        # Dados
        writer.writerows(transactions)
    
    print(f"‚úÖ {NUM_TRANSACTIONS} transa√ß√µes geradas!")
    print(f"üìÅ Arquivo salvo em: {OUTPUT_FILE}")
    
    # Estat√≠sticas
    print("\nüìä Estat√≠sticas dos dados gerados:")
    print(f"   - Per√≠odo: {start_date.strftime('%Y-%m-%d')} a {end_date.strftime('%Y-%m-%d')}")
    print(f"   - Produtos √∫nicos: {len(PRODUCTS)}")
    print(f"   - Categorias: {', '.join(CATEGORY_WEIGHTS.keys())}")
    print(f"   - Regi√µes: {', '.join(REGIONS)}")
    
    # Preview
    print("\nüìÑ Preview das primeiras linhas:")
    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i < 5:
                print(f"   {line.strip()}")
            else:
                break

def generate_products_catalog():
    """Gera cat√°logo de produtos separado"""
    catalog_file = "data/products.csv"
    
    with open(catalog_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["product_id", "product_name", "category", "base_price"])
        writer.writerows(PRODUCTS)
    
    print(f"‚úÖ Cat√°logo de produtos gerado: {catalog_file}")

def generate_text_data():
    """Gera arquivo de texto para exemplo de word count"""
    text_file = "data/input.txt"
    
    text_content = """Apache Spark is a unified analytics engine for large-scale data processing.
Spark provides high-level APIs in Java, Scala, Python and R.
PySpark is the Python API for Apache Spark.
Big Data processing has become essential in modern data science.
Data engineers use Spark to build robust data pipelines.
Machine Learning at scale is possible with Spark MLlib.
Real-time stream processing can be done with Spark Streaming.
Spark SQL provides a DataFrame API for structured data processing.
The Catalyst optimizer makes Spark SQL queries fast and efficient.
Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud.
Data scientists love PySpark for its simplicity and power.
Distributed computing enables processing of massive datasets.
Apache Spark revolutionized big data analytics.
In-memory computation makes Spark incredibly fast.
RDDs are the fundamental data structure in Spark.
DataFrames provide a higher-level abstraction than RDDs.
Lazy evaluation allows Spark to optimize the execution plan.
Transformations and actions are key concepts in Spark programming.
Spark applications can be written in multiple programming languages.
The Spark ecosystem includes MLlib, GraphX, and Spark Streaming."""
    
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(text_content)
    
    print(f"‚úÖ Arquivo de texto gerado: {text_file}")

if __name__ == "__main__":
    print("=" * 60)
    print("  GERADOR DE DADOS - PYSPARK LAB")
    print("=" * 60)
    print()
    
    # Gera todos os datasets
    generate_sales_data()
    print()
    generate_products_catalog()
    print()
    generate_text_data()
    
    print()
    print("=" * 60)
    print("‚úÖ Todos os arquivos foram gerados com sucesso!")
    print("=" * 60)
