#!/usr/bin/env python3
"""
Exemplo de Spark Streaming (Estruturado)
Demonstra processamento de dados em tempo real
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, window, count, sum, avg, explode, split, from_json
)
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
import time

def create_spark_session():
    """Cria SparkSession com suporte a streaming"""
    return SparkSession.builder \
        .appName("SparkStreamingExample") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .getOrCreate()

def streaming_word_count_example(spark):
    """
    Exemplo bÃ¡sico de streaming: Word Count em tempo real
    Monitora um diretÃ³rio por novos arquivos de texto
    """
    print("\n" + "="*60)
    print("ğŸ“¡ STREAMING EXAMPLE: Real-Time Word Count")
    print("="*60)
    
    # Define schema para os dados de entrada
    # Neste exemplo, lemos arquivos de texto linha por linha
    
    # Cria streaming DataFrame que monitora um diretÃ³rio
    lines_stream = spark.readStream \
        .format("text") \
        .option("maxFilesPerTrigger", 1) \
        .load("data/streaming_input/")
    
    # TransformaÃ§Ãµes (lazy, como sempre)
    words = lines_stream.select(
        explode(split(col("value"), " ")).alias("word")
    )
    
    word_counts = words.groupBy("word").count()
    
    # Inicia a query de streaming
    # Output para console
    query = word_counts.writeStream \
        .outputMode("complete") \
        .format("console") \
        .option("truncate", False) \
        .start()
    
    print("\nğŸ“Š Aguardando dados... (Adicione arquivos em data/streaming_input/)")
    print("   Pressione Ctrl+C para parar")
    
    # Aguarda tÃ©rmino (ou interrupÃ§Ã£o)
    try:
        query.awaitTermination(timeout=30)  # 30 segundos para demo
    except KeyboardInterrupt:
        query.stop()
        print("\nâ¹ï¸  Streaming interrompido")

def streaming_sales_aggregation(spark):
    """
    Exemplo avanÃ§ado: AgregaÃ§Ã£o de vendas em janelas de tempo
    Simula anÃ¡lise de vendas em tempo real
    """
    print("\n" + "="*60)
    print("ğŸ“¡ STREAMING EXAMPLE: Real-Time Sales Aggregation")
    print("="*60)
    
    # Define schema para dados de vendas
    schema = StructType([
        StructField("timestamp", StringType(), True),
        StructField("product_id", StringType(), True),
        StructField("category", StringType(), True),
        StructField("price", DoubleType(), True),
        StructField("quantity", IntegerType(), True),
        StructField("region", StringType(), True)
    ])
    
    # LÃª stream de arquivos CSV
    sales_stream = spark.readStream \
        .schema(schema) \
        .option("header", True) \
        .csv("data/streaming_sales/")
    
    # Adiciona coluna de receita
    sales_with_revenue = sales_stream.withColumn(
        "revenue",
        col("price") * col("quantity")
    )
    
    # AgregaÃ§Ã£o por janela de tempo (windowed aggregation)
    # Agrupa vendas em janelas de 10 segundos
    windowed_sales = sales_with_revenue \
        .groupBy(
            window(col("timestamp").cast("timestamp"), "10 seconds"),
            col("category")
        ) \
        .agg(
            count("*").alias("num_sales"),
            sum("revenue").alias("total_revenue"),
            avg("revenue").alias("avg_revenue")
        )
    
    # Escreve resultados
    query = windowed_sales.writeStream \
        .outputMode("update") \
        .format("console") \
        .option("truncate", False) \
        .start()
    
    print("\nğŸ“Š Processando vendas em tempo real...")
    print("   Janelas de 10 segundos")
    
    try:
        query.awaitTermination(timeout=60)
    except KeyboardInterrupt:
        query.stop()
        print("\nâ¹ï¸  Streaming interrompido")

def batch_vs_streaming_comparison():
    """
    Demonstra a diferenÃ§a entre processamento batch e streaming
    """
    print("\n" + "="*60)
    print("ğŸ“š BATCH vs STREAMING: ComparaÃ§Ã£o")
    print("="*60)
    
    comparison = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              BATCH vs STREAMING PROCESSING                 â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                            â•‘
    â•‘  BATCH PROCESSING (Tradicional)                           â•‘
    â•‘  â€¢ Processa dados histÃ³ricos completos                    â•‘
    â•‘  â€¢ ExecuÃ§Ã£o periÃ³dica (hourly, daily)                     â•‘
    â•‘  â€¢ Alta latÃªncia (minutos/horas)                          â•‘
    â•‘  â€¢ Exemplo: RelatÃ³rios diÃ¡rios de vendas                  â•‘
    â•‘                                                            â•‘
    â•‘  STREAMING PROCESSING (Tempo Real)                        â•‘
    â•‘  â€¢ Processa dados conforme chegam                         â•‘
    â•‘  â€¢ ExecuÃ§Ã£o contÃ­nua (near real-time)                     â•‘
    â•‘  â€¢ Baixa latÃªncia (segundos/milissegundos)                â•‘
    â•‘  â€¢ Exemplo: DetecÃ§Ã£o de fraude em tempo real              â•‘
    â•‘                                                            â•‘
    â•‘  SPARK STRUCTURED STREAMING                                â•‘
    â•‘  â€¢ Mesma API que batch (DataFrames)                       â•‘
    â•‘  â€¢ Processamento incremental                              â•‘
    â•‘  â€¢ Suporte a event-time e watermarking                    â•‘
    â•‘  â€¢ Exactly-once semantics                                 â•‘
    â•‘                                                            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    
    print(comparison)
    
    print("\nğŸ“Š Casos de Uso Comuns:")
    print("\n   BATCH:")
    print("     â€¢ ETL de data warehouses")
    print("     â€¢ RelatÃ³rios analÃ­ticos")
    print("     â€¢ Machine Learning training")
    print("     â€¢ AgregaÃ§Ãµes histÃ³ricas")
    
    print("\n   STREAMING:")
    print("     â€¢ Monitoramento de sistemas")
    print("     â€¢ DetecÃ§Ã£o de anomalias")
    print("     â€¢ RecomendaÃ§Ãµes em tempo real")
    print("     â€¢ Processamento de logs/eventos")
    print("     â€¢ IoT e sensores")

def create_sample_streaming_data():
    """Cria dados de exemplo para demonstraÃ§Ã£o de streaming"""
    import os
    
    # Cria diretÃ³rio
    os.makedirs("data/streaming_input", exist_ok=True)
    
    # Arquivo de exemplo
    sample_text = """
    Apache Spark Streaming example
    Real time data processing with PySpark
    Structured Streaming API demonstration
    """
    
    with open("data/streaming_input/sample.txt", "w") as f:
        f.write(sample_text)
    
    print("âœ… Dados de exemplo criados em data/streaming_input/")

def main():
    """FunÃ§Ã£o principal"""
    print("=" * 60)
    print("  SPARK STREAMING - EXEMPLOS E CONCEITOS")
    print("=" * 60)
    
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"\nâš™ï¸  Spark Version: {spark.version}")
    
    # Mostra comparaÃ§Ã£o conceitual
    batch_vs_streaming_comparison()
    
    print("\n" + "="*60)
    print("ğŸ“ NOTA SOBRE STREAMING")
    print("="*60)
    print("""
Este exemplo demonstra CONCEITOS de Spark Streaming.

Para executar streaming real, vocÃª precisaria:

1. Fonte de dados em streaming (Kafka, socket, files)
2. Infraestrutura adequada (cluster, recursos)
3. Caso de uso especÃ­fico em tempo real

O Spark Structured Streaming utiliza a MESMA API dos DataFrames,
mas com processamento incremental contÃ­nuo.

Principais diferenÃ§as na API:
  â€¢ spark.readStream vs spark.read
  â€¢ df.writeStream vs df.write
  â€¢ AgregaÃ§Ãµes com window()
  â€¢ Output modes: complete, append, update

Para este laboratÃ³rio, focamos em BATCH PROCESSING, que Ã©
mais adequado para ambiente de aprendizado e anÃ¡lises histÃ³ricas.
    """)
    
    print("\nğŸ“š Recursos para aprender mais sobre Streaming:")
    print("   â€¢ https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html")
    print("   â€¢ https://www.databricks.com/glossary/structured-streaming")
    
    spark.stop()
    print("\nğŸ”š SparkSession encerrada.")

if __name__ == "__main__":
    main()
