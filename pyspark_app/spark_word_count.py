#!/usr/bin/env python3
"""
Word Count com PySpark - Exemplo B√°sico
Demonstra conceitos fundamentais do Spark
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, lower, col, desc
import time

def create_spark_session():
    """Cria e configura SparkSession"""
    return SparkSession.builder \
        .appName("WordCount-PySpark") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()

def word_count_rdd_approach(spark, input_file):
    """
    Word Count usando RDDs (abordagem tradicional)
    Demonstra transforma√ß√µes e a√ß√µes
    """
    print("\n" + "="*60)
    print("üìä WORD COUNT - ABORDAGEM RDD")
    print("="*60)
    
    start_time = time.time()
    
    # L√™ arquivo como RDD
    text_rdd = spark.sparkContext.textFile(input_file)
    
    # Transforma√ß√µes (lazy)
    words_rdd = text_rdd \
        .flatMap(lambda line: line.lower().split()) \
        .filter(lambda word: len(word) > 3) \
        .map(lambda word: (word, 1)) \
        .reduceByKey(lambda a, b: a + b) \
        .sortBy(lambda x: x[1], ascending=False)
    
    # A√ß√£o (trigger execution)
    results = words_rdd.take(20)
    
    elapsed = time.time() - start_time
    
    # Exibe resultados
    print("\nüèÜ Top 20 palavras mais frequentes:")
    print(f"{'Palavra':<20} {'Contagem':>10}")
    print("-" * 32)
    for word, count in results:
        print(f"{word:<20} {count:>10}")
    
    print(f"\n‚è±Ô∏è  Tempo de execu√ß√£o: {elapsed:.3f} segundos")
    print(f"üìà Total de palavras √∫nicas: {words_rdd.count()}")

def word_count_dataframe_approach(spark, input_file):
    """
    Word Count usando DataFrames (abordagem moderna)
    Demonstra SQL API e otimiza√ß√µes do Catalyst
    """
    print("\n" + "="*60)
    print("üìä WORD COUNT - ABORDAGEM DATAFRAME")
    print("="*60)
    
    start_time = time.time()
    
    # L√™ arquivo como DataFrame
    df = spark.read.text(input_file)
    
    # Transforma√ß√µes usando DataFrame API
    words_df = df \
        .select(explode(split(lower(col("value")), r'\s+')).alias("word")) \
        .filter(col("word").rlike(r'^[a-z]{4,}$')) \
        .groupBy("word") \
        .count() \
        .orderBy(desc("count"))
    
    # A√ß√£o - mostra top 20
    results = words_df.limit(20)
    
    elapsed = time.time() - start_time
    
    print("\nüèÜ Top 20 palavras mais frequentes:")
    results.show(truncate=False)
    
    print(f"\n‚è±Ô∏è  Tempo de execu√ß√£o: {elapsed:.3f} segundos")
    print(f"üìà Total de palavras √∫nicas: {words_df.count()}")
    
    return words_df

def word_count_sql_approach(spark, input_file):
    """
    Word Count usando Spark SQL
    Demonstra a integra√ß√£o SQL
    """
    print("\n" + "="*60)
    print("üìä WORD COUNT - ABORDAGEM SQL")
    print("="*60)
    
    start_time = time.time()
    
    # L√™ arquivo e cria view tempor√°ria
    df = spark.read.text(input_file)
    df.createOrReplaceTempView("lines")
    
    # Query SQL
    sql_query = """
        SELECT word, COUNT(*) as count
        FROM (
            SELECT explode(split(lower(value), '\\s+')) as word
            FROM lines
        )
        WHERE length(word) > 3 AND word RLIKE '^[a-z]+$'
        GROUP BY word
        ORDER BY count DESC
        LIMIT 20
    """
    
    results = spark.sql(sql_query)
    
    elapsed = time.time() - start_time
    
    print("\nüèÜ Top 20 palavras mais frequentes:")
    results.show(truncate=False)
    
    print(f"\n‚è±Ô∏è  Tempo de execu√ß√£o: {elapsed:.3f} segundos")

def analyze_execution_plan(spark, input_file):
    """
    Analisa o plano de execu√ß√£o do Spark
    Demonstra o conceito de Lazy Evaluation
    """
    print("\n" + "="*60)
    print("üîç AN√ÅLISE DO PLANO DE EXECU√á√ÉO")
    print("="*60)
    
    df = spark.read.text(input_file)
    
    words_df = df \
        .select(explode(split(lower(col("value")), r'\s+')).alias("word")) \
        .filter(col("word").rlike(r'^[a-z]{4,}$')) \
        .groupBy("word") \
        .count() \
        .orderBy(desc("count"))
    
    print("\nüìã Logical Plan (antes das otimiza√ß√µes):")
    print("-" * 60)
    words_df.explain(mode="simple")
    
    print("\nüöÄ Physical Plan (plano otimizado pelo Catalyst):")
    print("-" * 60)
    words_df.explain(mode="formatted")

def main():
    """Fun√ß√£o principal"""
    print("=" * 60)
    print("  WORD COUNT COM PYSPARK")
    print("  Demonstra√ß√£o de RDDs, DataFrames e SQL")
    print("=" * 60)
    
    # Arquivo de entrada
    input_file = "data/input.txt"
    
    # Cria SparkSession
    spark = create_spark_session()
    
    # Configura log level para reduzir verbosidade
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"\nüìÇ Arquivo de entrada: {input_file}")
    print(f"‚öôÔ∏è  Spark Version: {spark.version}")
    print(f"üíª Cores dispon√≠veis: {spark.sparkContext.defaultParallelism}")
    
    try:
        # Executa diferentes abordagens
        word_count_rdd_approach(spark, input_file)
        word_count_dataframe_approach(spark, input_file)
        word_count_sql_approach(spark, input_file)
        analyze_execution_plan(spark, input_file)
        
        print("\n" + "="*60)
        print("‚úÖ An√°lise conclu√≠da com sucesso!")
        print("="*60)
        
        # Informa√ß√µes adicionais
        print("\nüìö Conceitos demonstrados:")
        print("   ‚Ä¢ RDDs: Transforma√ß√µes e a√ß√µes")
        print("   ‚Ä¢ DataFrames: API de alto n√≠vel")
        print("   ‚Ä¢ Spark SQL: Queries SQL em dados distribu√≠dos")
        print("   ‚Ä¢ Lazy Evaluation: Otimiza√ß√£o de execu√ß√£o")
        print("   ‚Ä¢ Catalyst Optimizer: Planos de execu√ß√£o otimizados")
        
    except FileNotFoundError:
        print(f"\n‚ùå Erro: Arquivo '{input_file}' n√£o encontrado!")
        print("üí° Execute primeiro: python3 data_generator.py")
    except Exception as e:
        print(f"\n‚ùå Erro durante execu√ß√£o: {e}")
    finally:
        # Encerra SparkSession
        spark.stop()
        print("\nüîö SparkSession encerrada.")

if __name__ == "__main__":
    main()
